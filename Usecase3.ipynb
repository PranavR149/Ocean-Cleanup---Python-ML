{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f449d65b",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "#  Use case 3:Predicting the most common trash  with the latitude and longitude "
=======
    "#  Use case 3:Forecasting Predominant Waste Types with Latitude and Longitude"
>>>>>>> c719119 (changes)
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad4e8ce",
   "metadata": {},
   "source": [
    "# Model:Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a3d6ec4",
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "ename": "InvalidParameterError",
     "evalue": "The 'max_features' parameter of DecisionTreeClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 39\u001b[0m\n\u001b[1;32m     29\u001b[0m dt_classifier \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m     30\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,  \u001b[38;5;66;03m# Allow the tree to be deeper\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     min_samples_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,  \u001b[38;5;66;03m# Require fewer samples to split a node\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     min_samples_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,  \u001b[38;5;66;03m# Require fewer samples at a leaf node\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Limit the number of features to consider for the best split\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     max_leaf_nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m ) \u001b[38;5;66;03m# Allow more leaf nodes)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Train the classifier\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m dt_classifier\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Predict on the test set\u001b[39;00m\n\u001b[1;32m     42\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m dt_classifier\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1145\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1141\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[1;32m   1142\u001b[0m )\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[0;32m-> 1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:638\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    631\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \n\u001b[1;32m    633\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 638\u001b[0m     validate_parameter_constraints(\n\u001b[1;32m    639\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameter_constraints,\n\u001b[1;32m    640\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m    641\u001b[0m         caller_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    642\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:96\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m     )\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m )\n",
      "\u001b[0;31mInvalidParameterError\u001b[0m: The 'max_features' parameter of DecisionTreeClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead."
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the classifier: 35.55%\n",
      "Training Accuracy: 35.19%\n",
      "Validation Accuracy: 35.55%\n",
      "Precision: 31.01%\n",
      "Recall: 35.55%\n",
      "F1 Score: 27.12%\n"
>>>>>>> c719119 (changes)
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'cleaned_ocean.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Columns to exclude that are not trash items\n",
    "excluded_columns = ['Total Items Collected', 'Adults', 'Children', 'People', 'Pounds', 'Miles','Year','Day','Month','Cleanup ID','Latitude','Longitude','Group Name','Zone','State']\n",
    "\n",
    "# Identify columns related to trash items collected based on data type and excluding the non-trash item columns\n",
    "trash_item_columns = [col for col in data.columns if data[col].dtype in ['int64', 'float64'] and col not in excluded_columns]\n",
    "\n",
    "# Determine the most common trash item for each cleanup event\n",
    "data['Most_Common_Trash'] = data[trash_item_columns].idxmax(axis=1)\n",
    "\n",
    "# Prepare the features and target\n",
    "X = data[['Latitude', 'Longitude']]  # Features\n",
    "y = data['Most_Common_Trash']  # Target\n",
    "\n",
    "# Splitting the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42,\n",
    "    max_depth=10,  # Allow the tree to be deeper\n",
    "    min_samples_split=20,  # Require fewer samples to split a node\n",
    "    min_samples_leaf=10,  # Require fewer samples at a leaf node\n",
    "    max_features='auto',  # Limit the number of features to consider for the best split\n",
    "    max_leaf_nodes=50 ) # Allow more leaf nodes)\n",
    "\n",
    "\n",
    "\n",
    "# Train the classifier\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Precision, Recall, and F1 Score, expressed as a percentage\n",
    "precision = precision_score(y_test, y_pred, average='weighted') * 100  \n",
    "recall = recall_score(y_test, y_pred, average='weighted') * 100\n",
    "f1 = f1_score(y_test, y_pred, average='weighted') * 100\n",
    "\n",
    "# Print out the results\n",
    "print(f\"Accuracy of the classifier: {accuracy:.2%}\")\n",
    "training_accuracy = accuracy_score(y_train, dt_classifier.predict(X_train))\n",
    "validation_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Training Accuracy: {training_accuracy:.2%}\")\n",
    "print(f\"Validation Accuracy: {validation_accuracy:.2%}\")\n",
    "\n",
    "print(f\"Precision: {precision:.2f}%\")\n",
    "print(f\"Recall: {recall:.2f}%\")\n",
    "print(f\"F1 Score: {f1:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "f39c99c2",
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 2,
   "id": "f39c99c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted most common trash item: Cigarette Butts\n"
     ]
    }
   ],
>>>>>>> c719119 (changes)
   "source": [
    "\n",
    "# Function to predict the most common trash item based on given latitude and longitude\n",
    "def predict_trash_item(latitude, longitude, model):\n",
    "    # Create a DataFrame with the input features\n",
    "    input_features = pd.DataFrame([[latitude, longitude]], columns=['Latitude', 'Longitude'])\n",
    "    \n",
    "    # Use the model to predict the most common trash item\n",
    "    prediction = model.predict(input_features)\n",
    "    \n",
    "    return prediction[0]\n",
    "\n",
    "# Example coordinates\n",
    "latitude_value = 38.187609  \n",
    "longitude_value = -121.974758  \n",
    "\n",
    "# Predict the trash item for the provided coordinates\n",
    "predicted_trash_item = predict_trash_item(latitude_value, longitude_value, dt_classifier)\n",
    "print(f\"Predicted most common trash item: {predicted_trash_item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358e9dca",
   "metadata": {},
   "source": [
    "# Model:Random Forest"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "efd1812e",
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 3,
   "id": "efd1812e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the classifier: 41.21%\n",
      "Training Accuracy: 45.58%\n",
      "Validation Accuracy: 41.21%\n",
      "Precision: 41.49%\n",
      "Recall: 41.21%\n",
      "F1 Score: 33.62%\n"
     ]
    }
   ],
>>>>>>> c719119 (changes)
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'cleaned_ocean.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Columns to exclude that are not trash items\n",
    "excluded_columns = ['Total Items Collected', 'Adults', 'Children', 'People', 'Pounds', 'Miles','Year','Day','Month','Cleanup ID','Latitude','Longitude','Group Name','Zone','State']\n",
    "\n",
    "# Identify columns related to trash items collected based on data type and excluding the non-trash item columns\n",
    "trash_item_columns = [col for col in data.columns if data[col].dtype in ['int64', 'float64'] and col not in excluded_columns]\n",
    "\n",
    "# Determine the most common trash item for each cleanup event\n",
    "data['Most_Common_Trash'] = data[trash_item_columns].idxmax(axis=1)\n",
    "\n",
    "# Prepare the features and target\n",
    "X = data[['Latitude', 'Longitude']]  # Features\n",
    "y = data['Most_Common_Trash']  # Target\n",
    "\n",
    "# Splitting the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "dt_classifier = RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=2, min_samples_leaf=1, random_state=42)\n",
    "#dt_classifier = RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_split=10, min_samples_leaf=5, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train the classifier\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Precision, Recall, and F1 Score, expressed as a percentage\n",
    "precision = precision_score(y_test, y_pred, average='weighted') * 100  # Multiplied by 100 to convert to percentage\n",
    "recall = recall_score(y_test, y_pred, average='weighted') * 100\n",
    "f1 = f1_score(y_test, y_pred, average='weighted') * 100\n",
    "\n",
    "training_accuracy = accuracy_score(y_train, dt_classifier.predict(X_train))\n",
    "validation_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print out the results\n",
    "print(f\"Accuracy of the classifier: {accuracy:.2%}\")\n",
    "print(f\"Training Accuracy: {training_accuracy:.2%}\")\n",
    "print(f\"Validation Accuracy: {validation_accuracy:.2%}\")\n",
    "print(f\"Precision: {precision:.2f}%\")\n",
    "print(f\"Recall: {recall:.2f}%\")\n",
    "print(f\"F1 Score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "5c297989",
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 4,
   "id": "5c297989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted most common trash item: Cigarette Butts\n"
     ]
    }
   ],
>>>>>>> c719119 (changes)
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'cleaned_ocean.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Columns to exclude that are not trash items\n",
    "excluded_columns = ['Total Items Collected', 'Adults', 'Children', 'People', 'Pounds', 'Miles','Year','Day','Month','Cleanup ID','Latitude','Longitude','Group Name','Zone','State']\n",
    "\n",
    "# Identify columns related to trash items collected based on data type and excluding the non-trash item columns\n",
    "trash_item_columns = [col for col in data.columns if data[col].dtype in ['int64', 'float64'] and col not in excluded_columns]\n",
    "\n",
    "# Determine the most common trash item for each cleanup event\n",
    "data['Most_Common_Trash'] = data[trash_item_columns].idxmax(axis=1)\n",
    "\n",
    "# Prepare the features and target\n",
    "X = data[['Latitude', 'Longitude']]  # Features\n",
    "y = data['Most_Common_Trash']  # Target\n",
    "\n",
    "# Splitting the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "dt_classifier = RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=2, min_samples_leaf=1, random_state=42)\n",
    "#dt_classifier = RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_split=10, min_samples_leaf=5, random_state=42)\n",
    "\n",
    "\n",
    "#dt_classifier = LinearSVC(C=1.0, random_state=42, dual=False)\n",
    "\n",
    "# Train the classifier\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "# Function to predict the most common trash item based on given latitude and longitude\n",
    "def predict_trash_item(latitude, longitude, model):\n",
    "    # Create a DataFrame with the input features\n",
    "    input_features = pd.DataFrame([[latitude, longitude]], columns=['Latitude', 'Longitude'])\n",
    "    \n",
    "    # Use the model to predict the most common trash item\n",
    "    prediction = model.predict(input_features)\n",
    "    \n",
    "    return prediction[0]\n",
    "\n",
    "# Example coordinates\n",
    "latitude_value = 38.187609  \n",
    "longitude_value = -121.974758 \n",
    "\n",
    "# Predict the trash item for the provided coordinates\n",
    "predicted_trash_item = predict_trash_item(latitude_value, longitude_value, dt_classifier)\n",
    "\n",
    "# Print out the results\n",
    "\n",
    "print(f\"Predicted most common trash item: {predicted_trash_item}\")\n"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": null,
   "id": "2dacd701",
   "metadata": {},
   "outputs": [],
=======
   "cell_type": "markdown",
   "id": "a682f7eb",
   "metadata": {},
   "source": [
    "# Finding best parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dacd701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}\n"
     ]
    }
   ],
>>>>>>> c719119 (changes)
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Use a subset of the data for faster experimentation\n",
    "X_train_sub, _, y_train_sub, _ = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a smaller grid of hyperparameters\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [None, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Enable parallelization with n_jobs\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train_sub, y_train_sub)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "## Traing and testing the model with best parameters\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "aa527ed5",
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 6,
   "id": "aa527ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted most common trash item: Grocery Bags (Plastic)\n",
      "Accuracy of the classifier: 42.63%\n",
      "Training Accuracy: 77.30%\n",
      "Validation Accuracy: 42.63%\n"
     ]
    }
   ],
>>>>>>> c719119 (changes)
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'cleaned_ocean.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Columns to exclude that are not trash items\n",
    "excluded_columns = ['Total Items Collected', 'Adults', 'Children', 'People', 'Pounds', 'Miles','Year','Day','Month','Cleanup ID','Latitude','Longitude','Group Name','Zone','State']\n",
    "\n",
    "# Identify columns related to trash items collected based on data type and excluding the non-trash item columns\n",
    "trash_item_columns = [col for col in data.columns if data[col].dtype in ['int64', 'float64'] and col not in excluded_columns]\n",
    "\n",
    "# Determine the most common trash item for each cleanup event\n",
    "data['Most_Common_Trash'] = data[trash_item_columns].idxmax(axis=1)\n",
    "\n",
    "# Prepare the features and target\n",
    "X = data[['Latitude', 'Longitude']]  # Features\n",
    "y = data['Most_Common_Trash']  # Target\n",
    "\n",
    "# Splitting the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "dt_classifier = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=5, min_samples_leaf=2, random_state=42)\n",
    "#dt_classifier = RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_split=10, min_samples_leaf=5, random_state=42)\n",
    "\n",
    "\n",
    "#dt_classifier = LinearSVC(C=1.0, random_state=42, dual=False)\n",
    "\n",
    "# Train the classifier\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "# Function to predict the most common trash item based on given latitude and longitude\n",
    "def predict_trash_item(latitude, longitude, model):\n",
    "    # Create a DataFrame with the input features\n",
    "    input_features = pd.DataFrame([[latitude, longitude]], columns=['Latitude', 'Longitude'])\n",
    "    \n",
    "    # Use the model to predict the most common trash item\n",
    "    prediction = model.predict(input_features)\n",
    "    \n",
    "    return prediction[0]\n",
    "\n",
    "# Example coordinates\n",
    "latitude_value = 38.187609  \n",
    "longitude_value = -121.974758  \n",
    "\n",
    "# Predict the trash item for the provided coordinates\n",
    "predicted_trash_item = predict_trash_item(latitude_value, longitude_value, dt_classifier)\n",
    "\n",
    "# Print out the results\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "training_accuracy = accuracy_score(y_train, dt_classifier.predict(X_train))\n",
    "validation_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Predicted most common trash item: {predicted_trash_item}\")\n",
    "print(f\"Accuracy of the classifier: {accuracy:.2%}\")\n",
    "print(f\"Training Accuracy: {training_accuracy:.2%}\")\n",
    "print(f\"Validation Accuracy: {validation_accuracy:.2%}\")\n"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": null,
   "id": "1727bf62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a7d29",
   "metadata": {},
   "outputs": [],
=======
   "cell_type": "markdown",
   "id": "068eed21",
   "metadata": {},
   "source": [
    "# Finding best accuracy at each epoch through early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d85a7d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Accuracy: 42.59%\n",
      "Epoch 2, Accuracy: 42.59%\n",
      "Epoch 3, Accuracy: 42.59%\n",
      "Epoch 4, Accuracy: 42.59%\n",
      "Epoch 5, Accuracy: 42.59%\n",
      "Epoch 6, Accuracy: 42.59%\n",
      "Epoch 7, Accuracy: 42.59%\n",
      "Epoch 8, Accuracy: 42.59%\n",
      "Epoch 9, Accuracy: 42.59%\n",
      "Epoch 10, Accuracy: 42.59%\n",
      "Epoch 11, Accuracy: 42.59%\n",
      "Early stopping at epoch 11 with the best accuracy of 42.59%\n"
     ]
    }
   ],
>>>>>>> c719119 (changes)
   "source": [
    "# Initialize the RandomForestClassifier with early stopping\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=5, min_samples_leaf=1, random_state=42)\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_round = 0\n",
    "early_stopping_rounds = 10\n",
    "\n",
    "for epoch in range(100):  \n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Print the accuracy for each epoch\n",
    "    print(f\"Epoch {epoch + 1}, Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_round = epoch + 1\n",
    "    elif epoch + 1 - best_round >= early_stopping_rounds:\n",
    "        print(f\"Early stopping at epoch {epoch + 1} with the best accuracy of {best_accuracy:.2%}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "49e1d3f8",
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 8,
   "id": "49e1d3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted most common trash item: Grocery Bags (Plastic)\n"
     ]
    }
   ],
>>>>>>> c719119 (changes)
   "source": [
    "# Function to predict the most common trash item based on given latitude and longitude\n",
    "def predict_trash_item(latitude, longitude, model):\n",
    "    # Create a DataFrame with the input features\n",
    "    input_features = pd.DataFrame([[latitude, longitude]], columns=['Latitude', 'Longitude'])\n",
    "    \n",
    "    # Use the model to predict the most common trash item\n",
    "    prediction = model.predict(input_features)\n",
    "    \n",
    "    return prediction[0]\n",
    "\n",
    "# Example coordinates\n",
    "latitude_value = 38.187609  \n",
    "longitude_value = -121.974758  \n",
    "\n",
    "# Predict the trash item for the provided coordinates\n",
    "predicted_trash_item = predict_trash_item(latitude_value, longitude_value, dt_classifier)\n",
    "\n",
    "# Print out the results\n",
    "print(f\"Predicted most common trash item: {predicted_trash_item}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f59c992",
   "metadata": {},
   "source": [
    "# Model 3:Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "0716895c",
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 9,
   "id": "0716895c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the classifier: 31.02%\n",
      "Training Accuracy: 30.36%\n",
      "Validation Accuracy: 31.02%\n",
      "Precision: 16.16%\n",
      "Recall: 31.02%\n",
      "F1 Score: 20.95%\n"
     ]
    }
   ],
>>>>>>> c719119 (changes)
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'cleaned_ocean.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Columns to exclude that are not trash items\n",
    "excluded_columns = ['Total Items Collected', 'Adults', 'Children', 'People', 'Pounds', 'Miles', 'Year', 'Day', 'Month', 'Cleanup ID', 'Latitude', 'Longitude', 'Group Name', 'Zone', 'State']\n",
    "\n",
    "# Identify columns related to trash items collected based on data type and excluding the non-trash item columns\n",
    "trash_item_columns = [col for col in data.columns if data[col].dtype in ['int64', 'float64'] and col not in excluded_columns]\n",
    "\n",
    "# Determine the most common trash item for each cleanup event\n",
    "data['Most_Common_Trash'] = data[trash_item_columns].idxmax(axis=1)\n",
    "\n",
    "# Prepare the features and target\n",
    "X = data[['Latitude', 'Longitude']]  # Features\n",
    "y = data['Most_Common_Trash']  # Target\n",
    "\n",
    "# Splitting the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Logistic Regression Classifier\n",
    "logistic_classifier = LogisticRegression(random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "logistic_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = logistic_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Precision, Recall, and F1 Score, expressed as a percentage\n",
    "precision = precision_score(y_test, y_pred, average='weighted') * 100  # Multiplied by 100 to convert to percentage\n",
    "recall = recall_score(y_test, y_pred, average='weighted') * 100\n",
    "f1 = f1_score(y_test, y_pred, average='weighted') * 100\n",
    "\n",
    "# Print out the results\n",
    "print(f\"Accuracy of the classifier: {accuracy:.2%}\")\n",
    "training_accuracy = accuracy_score(y_train, logistic_classifier.predict(X_train))\n",
    "validation_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Training Accuracy: {training_accuracy:.2%}\")\n",
    "print(f\"Validation Accuracy: {validation_accuracy:.2%}\")\n",
    "\n",
    "print(f\"Precision: {precision:.2f}%\")\n",
    "print(f\"Recall: {recall:.2f}%\")\n",
    "print(f\"F1 Score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "c1a44bbc",
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 10,
   "id": "c1a44bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted most common trash item: Plastic Pieces\n"
     ]
    }
   ],
>>>>>>> c719119 (changes)
   "source": [
    "# Function to predict the most common trash item based on given latitude and longitude using Logistic Regression\n",
    "def predict_trash_item(latitude, longitude, model):\n",
    "    # Create a DataFrame with the input features\n",
    "    input_features = pd.DataFrame([[latitude, longitude]], columns=['Latitude', 'Longitude'])\n",
    "    \n",
    "    # Use the model to predict the most common trash item\n",
    "    prediction = model.predict(input_features)\n",
    "    \n",
    "    return prediction[0]\n",
    "\n",
    "# Example coordinates\n",
    "latitude_value = 38.187609  # Replace with actual latitude\n",
    "longitude_value = -121.974758  # Replace with actual longitude\n",
    "\n",
    "# Predict the trash item for the provided coordinates using Logistic Regression model\n",
    "predicted_trash_item = predict_trash_item(latitude_value, longitude_value, logistic_classifier)\n",
    "print(f\"Predicted most common trash item: {predicted_trash_item}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fe3919",
   "metadata": {},
   "source": [
    "# Model 4 :KNN"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "3e2ea7b9",
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 11,
   "id": "3e2ea7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the classifier: 39.92%\n",
      "Training Accuracy: 55.09%\n",
      "Validation Accuracy: 39.92%\n",
      "Precision: 37.99%\n",
      "Recall: 39.92%\n",
      "F1 Score: 38.32%\n"
     ]
    }
   ],
>>>>>>> c719119 (changes)
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'cleaned_ocean.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Columns to exclude that are not trash items\n",
    "excluded_columns = ['Total Items Collected', 'Adults', 'Children', 'People', 'Pounds', 'Miles', 'Year', 'Day', 'Month', 'Cleanup ID', 'Latitude', 'Longitude', 'Group Name', 'Zone', 'State']\n",
    "\n",
    "# Identify columns related to trash items collected based on data type and excluding the non-trash item columns\n",
    "trash_item_columns = [col for col in data.columns if data[col].dtype in ['int64', 'float64'] and col not in excluded_columns]\n",
    "\n",
    "# Determine the most common trash item for each cleanup event\n",
    "data['Most_Common_Trash'] = data[trash_item_columns].idxmax(axis=1)\n",
    "\n",
    "# Prepare the features and target\n",
    "X = data[['Latitude', 'Longitude']] \n",
    "y = data['Most_Common_Trash']  \n",
    "\n",
    "# Splitting the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the KNN Classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors (k) as needed\n",
    "\n",
    "# Train the classifier\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Precision, Recall, and F1 Score, expressed as a percentage\n",
    "precision = precision_score(y_test, y_pred, average='weighted') * 100  # Multiplied by 100 to convert to percentage\n",
    "recall = recall_score(y_test, y_pred, average='weighted') * 100\n",
    "f1 = f1_score(y_test, y_pred, average='weighted') * 100\n",
    "# Print out the results\n",
    "print(f\"Accuracy of the classifier: {accuracy:.2%}\")\n",
    "training_accuracy = accuracy_score(y_train, knn_classifier.predict(X_train))\n",
    "validation_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Training Accuracy: {training_accuracy:.2%}\")\n",
    "print(f\"Validation Accuracy: {validation_accuracy:.2%}\")\n",
    "\n",
    "print(f\"Precision: {precision:.2f}%\")\n",
    "print(f\"Recall: {recall:.2f}%\")\n",
    "print(f\"F1 Score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "6341c676",
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 12,
   "id": "6341c676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted most common trash item using KNN: Grocery Bags (Plastic)\n"
     ]
    }
   ],
>>>>>>> c719119 (changes)
   "source": [
    "# Function to predict the most common trash item based on given latitude and longitude using KNN\n",
    "def predict_trash_item_knn(latitude, longitude, model):\n",
    "    # Create a DataFrame with the input features\n",
    "    input_features = pd.DataFrame([[latitude, longitude]], columns=['Latitude', 'Longitude'])\n",
    "    \n",
    "    # Use the model to predict the most common trash item\n",
    "    prediction = model.predict(input_features)\n",
    "    \n",
    "    return prediction[0]\n",
    "\n",
    "# Example coordinates\n",
    "latitude_value = 38.187609  # Replace with actual latitude\n",
    "longitude_value = -121.974758  # Replace with actual longitude\n",
    "\n",
    "# Predict the trash item for the provided coordinates using KNN model\n",
    "predicted_trash_item_knn = predict_trash_item_knn(latitude_value, longitude_value, knn_classifier)\n",
    "print(f\"Predicted most common trash item using KNN: {predicted_trash_item_knn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6829ef",
   "metadata": {},
   "source": [
    "# Model 5 :SVM"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "b049de09",
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 13,
   "id": "b049de09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the classifier: 29.60%\n",
      "Training Accuracy: 29.01%\n",
      "Validation Accuracy: 29.60%\n",
      "Precision: 15.42%\n",
      "Recall: 29.60%\n",
      "F1 Score: 19.83%\n"
     ]
    }
   ],
>>>>>>> c719119 (changes)
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'cleaned_ocean.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Columns to exclude that are not trash items\n",
    "excluded_columns = [\n",
    "    'Total Items Collected', 'Adults', 'Children', 'People', 'Pounds', 'Miles', \n",
    "    'Year', 'Day', 'Month', 'Cleanup ID', 'Latitude', 'Longitude', 'Group Name', \n",
    "    'Zone', 'State'\n",
    "]\n",
    "\n",
    "# Identify columns related to trash items collected based on data type and excluding the non-trash item columns\n",
    "trash_item_columns = [col for col in data.columns if data[col].dtype in ['int64', 'float64'] and col not in excluded_columns]\n",
    "\n",
    "# Determine the most common trash item for each cleanup event\n",
    "data['Most_Common_Trash'] = data[trash_item_columns].idxmax(axis=1)\n",
    "\n",
    "# Prepare the features and target\n",
    "X = data[['Latitude', 'Longitude']]  # Features\n",
    "y = data['Most_Common_Trash']  # Target\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Reduce the data using PCA\n",
    "pca = PCA(n_components=2)  # Since we only have 2 features, we'll keep them both\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Splitting the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the SVM Classifier with optimized parameters\n",
    "svm_classifier = SVC(kernel='linear', C=1.0, random_state=42, cache_size=7000)  # Increased cache size\n",
    "\n",
    "# Train the classifier\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print out the results\n",
    "accuracy, training_accuracy, validation_accuracy = (\n",
    "    accuracy_score(y_test, y_pred),\n",
    "    accuracy_score(y_train, svm_classifier.predict(X_train)),\n",
    "    accuracy_score(y_test, y_pred)\n",
    ")\n",
    "\n",
    "accuracy, training_accuracy, validation_accuracy\n",
    "\n",
    "\n",
    "# Calculate Precision, Recall, and F1 Score\n",
    "precision = precision_score(y_test, y_pred, average='weighted') * 100\n",
    "recall = recall_score(y_test, y_pred, average='weighted') * 100\n",
    "f1 = f1_score(y_test, y_pred, average='weighted') * 100\n",
    "\n",
    "# Print out the results with accuracy, precision, recall, and F1 score\n",
    "print(f\"Accuracy of the classifier: {accuracy*100:.2f}%\")\n",
    "print(f\"Training Accuracy: {training_accuracy*100:.2f}%\")\n",
    "print(f\"Validation Accuracy: {validation_accuracy*100:.2f}%\")\n",
    "print(f\"Precision: {precision:.2f}%\")\n",
    "print(f\"Recall: {recall:.2f}%\")\n",
    "print(f\"F1 Score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "14d2074e",
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 14,
   "id": "14d2074e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted most common trash item using SVM: Cigarette Butts\n"
     ]
    }
   ],
>>>>>>> c719119 (changes)
   "source": [
    "# Function to predict the most common trash item based on given latitude and longitude using SVM\n",
    "def predict_trash_item_svm(latitude, longitude, model):\n",
    "    # Create a DataFrame with the input features\n",
    "    input_features = pd.DataFrame([[latitude, longitude]], columns=['Latitude', 'Longitude'])\n",
    "    \n",
    "    # Use the model to predict the most common trash item\n",
    "    prediction = model.predict(input_features)\n",
    "    \n",
    "    return prediction[0]\n",
    "\n",
    "# Example coordinates\n",
    "latitude_value = 38.187609  \n",
    "longitude_value = -121.974758  \n",
    "\n",
    "# Predict the trash item for the provided coordinates using SVM model\n",
    "predicted_trash_item_svm = predict_trash_item_svm(latitude_value, longitude_value, svm_classifier)\n",
    "print(f\"Predicted most common trash item using SVM: {predicted_trash_item_svm}\")\n"
   ]
<<<<<<< HEAD
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa44d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Preparing data for the bubble chart\n",
    "data['Bubble Size'] = data['Plastic Pieces']  # Modify as per actual data for different trash type\n",
    "max_bubble_size = 100  # Maximum bubble size, can be adjusted\n",
    "\n",
    "# Normalizing the bubble sizes for better visualization\n",
    "data['Normalized Bubble Size'] = (data['Bubble Size'] / data['Bubble Size'].max()) * max_bubble_size\n",
    "\n",
    "# Creating the bubble chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(data['Longitude'], data['Latitude'], s=data['Normalized Bubble Size'], alpha=0.5)\n",
    "plt.title('Bubble Chart of Cleanup Locations')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.colorbar(label='Plastic Pieces Count (Normalized)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd92c20b",
   "metadata": {},
   "outputs": [],
   "source": []
=======
>>>>>>> c719119 (changes)
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.11.4"
=======
   "version": "3.9.13"
>>>>>>> c719119 (changes)
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
