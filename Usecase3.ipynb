{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f449d65b",
   "metadata": {},
   "source": [
    "#  Use case 3:Forecasting Predominant Waste Types with Latitude and Longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad4e8ce",
   "metadata": {},
   "source": [
    "# Model:Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a3d6ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the classifier: 35.55%\n",
      "Training Accuracy: 35.19%\n",
      "Validation Accuracy: 35.55%\n",
      "Precision: 31.01%\n",
      "Recall: 35.55%\n",
      "F1 Score: 27.12%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'cleaned_ocean.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Columns to exclude that are not trash items\n",
    "excluded_columns = ['Total Items Collected', 'Adults', 'Children', 'People', 'Pounds', 'Miles','Year','Day','Month','Cleanup ID','Latitude','Longitude','Group Name','Zone','State']\n",
    "\n",
    "# Identify columns related to trash items collected based on data type and excluding the non-trash item columns\n",
    "trash_item_columns = [col for col in data.columns if data[col].dtype in ['int64', 'float64'] and col not in excluded_columns]\n",
    "\n",
    "# Determine the most common trash item for each cleanup event\n",
    "data['Most_Common_Trash'] = data[trash_item_columns].idxmax(axis=1)\n",
    "\n",
    "# Prepare the features and target\n",
    "X = data[['Latitude', 'Longitude']]  # Features\n",
    "y = data['Most_Common_Trash']  # Target\n",
    "\n",
    "# Splitting the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42,\n",
    "    max_depth=10,  # Allow the tree to be deeper\n",
    "    min_samples_split=20,  # Require fewer samples to split a node\n",
    "    min_samples_leaf=10,  # Require fewer samples at a leaf node\n",
    "    max_features='auto',  # Limit the number of features to consider for the best split\n",
    "    max_leaf_nodes=50 ) # Allow more leaf nodes)\n",
    "\n",
    "\n",
    "\n",
    "# Train the classifier\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Precision, Recall, and F1 Score, expressed as a percentage\n",
    "precision = precision_score(y_test, y_pred, average='weighted') * 100  \n",
    "recall = recall_score(y_test, y_pred, average='weighted') * 100\n",
    "f1 = f1_score(y_test, y_pred, average='weighted') * 100\n",
    "\n",
    "# Print out the results\n",
    "print(f\"Accuracy of the classifier: {accuracy:.2%}\")\n",
    "training_accuracy = accuracy_score(y_train, dt_classifier.predict(X_train))\n",
    "validation_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Training Accuracy: {training_accuracy:.2%}\")\n",
    "print(f\"Validation Accuracy: {validation_accuracy:.2%}\")\n",
    "\n",
    "print(f\"Precision: {precision:.2f}%\")\n",
    "print(f\"Recall: {recall:.2f}%\")\n",
    "print(f\"F1 Score: {f1:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f39c99c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted most common trash item: Cigarette Butts\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to predict the most common trash item based on given latitude and longitude\n",
    "def predict_trash_item(latitude, longitude, model):\n",
    "    # Create a DataFrame with the input features\n",
    "    input_features = pd.DataFrame([[latitude, longitude]], columns=['Latitude', 'Longitude'])\n",
    "    \n",
    "    # Use the model to predict the most common trash item\n",
    "    prediction = model.predict(input_features)\n",
    "    \n",
    "    return prediction[0]\n",
    "\n",
    "# Example coordinates\n",
    "latitude_value = 38.187609  \n",
    "longitude_value = -121.974758  \n",
    "\n",
    "# Predict the trash item for the provided coordinates\n",
    "predicted_trash_item = predict_trash_item(latitude_value, longitude_value, dt_classifier)\n",
    "print(f\"Predicted most common trash item: {predicted_trash_item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358e9dca",
   "metadata": {},
   "source": [
    "# Model:Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efd1812e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the classifier: 41.21%\n",
      "Training Accuracy: 45.58%\n",
      "Validation Accuracy: 41.21%\n",
      "Precision: 41.49%\n",
      "Recall: 41.21%\n",
      "F1 Score: 33.62%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'cleaned_ocean.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Columns to exclude that are not trash items\n",
    "excluded_columns = ['Total Items Collected', 'Adults', 'Children', 'People', 'Pounds', 'Miles','Year','Day','Month','Cleanup ID','Latitude','Longitude','Group Name','Zone','State']\n",
    "\n",
    "# Identify columns related to trash items collected based on data type and excluding the non-trash item columns\n",
    "trash_item_columns = [col for col in data.columns if data[col].dtype in ['int64', 'float64'] and col not in excluded_columns]\n",
    "\n",
    "# Determine the most common trash item for each cleanup event\n",
    "data['Most_Common_Trash'] = data[trash_item_columns].idxmax(axis=1)\n",
    "\n",
    "# Prepare the features and target\n",
    "X = data[['Latitude', 'Longitude']]  # Features\n",
    "y = data['Most_Common_Trash']  # Target\n",
    "\n",
    "# Splitting the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "dt_classifier = RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=2, min_samples_leaf=1, random_state=42)\n",
    "#dt_classifier = RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_split=10, min_samples_leaf=5, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train the classifier\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Precision, Recall, and F1 Score, expressed as a percentage\n",
    "precision = precision_score(y_test, y_pred, average='weighted') * 100  # Multiplied by 100 to convert to percentage\n",
    "recall = recall_score(y_test, y_pred, average='weighted') * 100\n",
    "f1 = f1_score(y_test, y_pred, average='weighted') * 100\n",
    "\n",
    "training_accuracy = accuracy_score(y_train, dt_classifier.predict(X_train))\n",
    "validation_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print out the results\n",
    "print(f\"Accuracy of the classifier: {accuracy:.2%}\")\n",
    "print(f\"Training Accuracy: {training_accuracy:.2%}\")\n",
    "print(f\"Validation Accuracy: {validation_accuracy:.2%}\")\n",
    "print(f\"Precision: {precision:.2f}%\")\n",
    "print(f\"Recall: {recall:.2f}%\")\n",
    "print(f\"F1 Score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c297989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted most common trash item: Cigarette Butts\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'cleaned_ocean.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Columns to exclude that are not trash items\n",
    "excluded_columns = ['Total Items Collected', 'Adults', 'Children', 'People', 'Pounds', 'Miles','Year','Day','Month','Cleanup ID','Latitude','Longitude','Group Name','Zone','State']\n",
    "\n",
    "# Identify columns related to trash items collected based on data type and excluding the non-trash item columns\n",
    "trash_item_columns = [col for col in data.columns if data[col].dtype in ['int64', 'float64'] and col not in excluded_columns]\n",
    "\n",
    "# Determine the most common trash item for each cleanup event\n",
    "data['Most_Common_Trash'] = data[trash_item_columns].idxmax(axis=1)\n",
    "\n",
    "# Prepare the features and target\n",
    "X = data[['Latitude', 'Longitude']]  # Features\n",
    "y = data['Most_Common_Trash']  # Target\n",
    "\n",
    "# Splitting the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "dt_classifier = RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=2, min_samples_leaf=1, random_state=42)\n",
    "#dt_classifier = RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_split=10, min_samples_leaf=5, random_state=42)\n",
    "\n",
    "\n",
    "#dt_classifier = LinearSVC(C=1.0, random_state=42, dual=False)\n",
    "\n",
    "# Train the classifier\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "# Function to predict the most common trash item based on given latitude and longitude\n",
    "def predict_trash_item(latitude, longitude, model):\n",
    "    # Create a DataFrame with the input features\n",
    "    input_features = pd.DataFrame([[latitude, longitude]], columns=['Latitude', 'Longitude'])\n",
    "    \n",
    "    # Use the model to predict the most common trash item\n",
    "    prediction = model.predict(input_features)\n",
    "    \n",
    "    return prediction[0]\n",
    "\n",
    "# Example coordinates\n",
    "latitude_value = 38.187609  \n",
    "longitude_value = -121.974758 \n",
    "\n",
    "# Predict the trash item for the provided coordinates\n",
    "predicted_trash_item = predict_trash_item(latitude_value, longitude_value, dt_classifier)\n",
    "\n",
    "# Print out the results\n",
    "\n",
    "print(f\"Predicted most common trash item: {predicted_trash_item}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a682f7eb",
   "metadata": {},
   "source": [
    "# Finding best parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dacd701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Use a subset of the data for faster experimentation\n",
    "X_train_sub, _, y_train_sub, _ = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a smaller grid of hyperparameters\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [None, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Enable parallelization with n_jobs\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train_sub, y_train_sub)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "## Traing and testing the model with best parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa527ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted most common trash item: Grocery Bags (Plastic)\n",
      "Accuracy of the classifier: 42.63%\n",
      "Training Accuracy: 77.30%\n",
      "Validation Accuracy: 42.63%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'cleaned_ocean.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Columns to exclude that are not trash items\n",
    "excluded_columns = ['Total Items Collected', 'Adults', 'Children', 'People', 'Pounds', 'Miles','Year','Day','Month','Cleanup ID','Latitude','Longitude','Group Name','Zone','State']\n",
    "\n",
    "# Identify columns related to trash items collected based on data type and excluding the non-trash item columns\n",
    "trash_item_columns = [col for col in data.columns if data[col].dtype in ['int64', 'float64'] and col not in excluded_columns]\n",
    "\n",
    "# Determine the most common trash item for each cleanup event\n",
    "data['Most_Common_Trash'] = data[trash_item_columns].idxmax(axis=1)\n",
    "\n",
    "# Prepare the features and target\n",
    "X = data[['Latitude', 'Longitude']]  # Features\n",
    "y = data['Most_Common_Trash']  # Target\n",
    "\n",
    "# Splitting the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "dt_classifier = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=5, min_samples_leaf=2, random_state=42)\n",
    "#dt_classifier = RandomForestClassifier(n_estimators=100, max_depth=5, min_samples_split=10, min_samples_leaf=5, random_state=42)\n",
    "\n",
    "\n",
    "#dt_classifier = LinearSVC(C=1.0, random_state=42, dual=False)\n",
    "\n",
    "# Train the classifier\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "# Function to predict the most common trash item based on given latitude and longitude\n",
    "def predict_trash_item(latitude, longitude, model):\n",
    "    # Create a DataFrame with the input features\n",
    "    input_features = pd.DataFrame([[latitude, longitude]], columns=['Latitude', 'Longitude'])\n",
    "    \n",
    "    # Use the model to predict the most common trash item\n",
    "    prediction = model.predict(input_features)\n",
    "    \n",
    "    return prediction[0]\n",
    "\n",
    "# Example coordinates\n",
    "latitude_value = 38.187609  \n",
    "longitude_value = -121.974758  \n",
    "\n",
    "# Predict the trash item for the provided coordinates\n",
    "predicted_trash_item = predict_trash_item(latitude_value, longitude_value, dt_classifier)\n",
    "\n",
    "# Print out the results\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "training_accuracy = accuracy_score(y_train, dt_classifier.predict(X_train))\n",
    "validation_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Predicted most common trash item: {predicted_trash_item}\")\n",
    "print(f\"Accuracy of the classifier: {accuracy:.2%}\")\n",
    "print(f\"Training Accuracy: {training_accuracy:.2%}\")\n",
    "print(f\"Validation Accuracy: {validation_accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068eed21",
   "metadata": {},
   "source": [
    "# Finding best accuracy at each epoch through early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d85a7d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Accuracy: 42.59%\n",
      "Epoch 2, Accuracy: 42.59%\n",
      "Epoch 3, Accuracy: 42.59%\n",
      "Epoch 4, Accuracy: 42.59%\n",
      "Epoch 5, Accuracy: 42.59%\n",
      "Epoch 6, Accuracy: 42.59%\n",
      "Epoch 7, Accuracy: 42.59%\n",
      "Epoch 8, Accuracy: 42.59%\n",
      "Epoch 9, Accuracy: 42.59%\n",
      "Epoch 10, Accuracy: 42.59%\n",
      "Epoch 11, Accuracy: 42.59%\n",
      "Early stopping at epoch 11 with the best accuracy of 42.59%\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RandomForestClassifier with early stopping\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=5, min_samples_leaf=1, random_state=42)\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_round = 0\n",
    "early_stopping_rounds = 10\n",
    "\n",
    "for epoch in range(100):  \n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Print the accuracy for each epoch\n",
    "    print(f\"Epoch {epoch + 1}, Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_round = epoch + 1\n",
    "    elif epoch + 1 - best_round >= early_stopping_rounds:\n",
    "        print(f\"Early stopping at epoch {epoch + 1} with the best accuracy of {best_accuracy:.2%}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49e1d3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted most common trash item: Grocery Bags (Plastic)\n"
     ]
    }
   ],
   "source": [
    "# Function to predict the most common trash item based on given latitude and longitude\n",
    "def predict_trash_item(latitude, longitude, model):\n",
    "    # Create a DataFrame with the input features\n",
    "    input_features = pd.DataFrame([[latitude, longitude]], columns=['Latitude', 'Longitude'])\n",
    "    \n",
    "    # Use the model to predict the most common trash item\n",
    "    prediction = model.predict(input_features)\n",
    "    \n",
    "    return prediction[0]\n",
    "\n",
    "# Example coordinates\n",
    "latitude_value = 38.187609  \n",
    "longitude_value = -121.974758  \n",
    "\n",
    "# Predict the trash item for the provided coordinates\n",
    "predicted_trash_item = predict_trash_item(latitude_value, longitude_value, dt_classifier)\n",
    "\n",
    "# Print out the results\n",
    "print(f\"Predicted most common trash item: {predicted_trash_item}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f59c992",
   "metadata": {},
   "source": [
    "# Model 3:Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0716895c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the classifier: 31.02%\n",
      "Training Accuracy: 30.36%\n",
      "Validation Accuracy: 31.02%\n",
      "Precision: 16.16%\n",
      "Recall: 31.02%\n",
      "F1 Score: 20.95%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'cleaned_ocean.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Columns to exclude that are not trash items\n",
    "excluded_columns = ['Total Items Collected', 'Adults', 'Children', 'People', 'Pounds', 'Miles', 'Year', 'Day', 'Month', 'Cleanup ID', 'Latitude', 'Longitude', 'Group Name', 'Zone', 'State']\n",
    "\n",
    "# Identify columns related to trash items collected based on data type and excluding the non-trash item columns\n",
    "trash_item_columns = [col for col in data.columns if data[col].dtype in ['int64', 'float64'] and col not in excluded_columns]\n",
    "\n",
    "# Determine the most common trash item for each cleanup event\n",
    "data['Most_Common_Trash'] = data[trash_item_columns].idxmax(axis=1)\n",
    "\n",
    "# Prepare the features and target\n",
    "X = data[['Latitude', 'Longitude']]  # Features\n",
    "y = data['Most_Common_Trash']  # Target\n",
    "\n",
    "# Splitting the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Logistic Regression Classifier\n",
    "logistic_classifier = LogisticRegression(random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "logistic_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = logistic_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Precision, Recall, and F1 Score, expressed as a percentage\n",
    "precision = precision_score(y_test, y_pred, average='weighted') * 100  # Multiplied by 100 to convert to percentage\n",
    "recall = recall_score(y_test, y_pred, average='weighted') * 100\n",
    "f1 = f1_score(y_test, y_pred, average='weighted') * 100\n",
    "\n",
    "# Print out the results\n",
    "print(f\"Accuracy of the classifier: {accuracy:.2%}\")\n",
    "training_accuracy = accuracy_score(y_train, logistic_classifier.predict(X_train))\n",
    "validation_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Training Accuracy: {training_accuracy:.2%}\")\n",
    "print(f\"Validation Accuracy: {validation_accuracy:.2%}\")\n",
    "\n",
    "print(f\"Precision: {precision:.2f}%\")\n",
    "print(f\"Recall: {recall:.2f}%\")\n",
    "print(f\"F1 Score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1a44bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted most common trash item: Plastic Pieces\n"
     ]
    }
   ],
   "source": [
    "# Function to predict the most common trash item based on given latitude and longitude using Logistic Regression\n",
    "def predict_trash_item(latitude, longitude, model):\n",
    "    # Create a DataFrame with the input features\n",
    "    input_features = pd.DataFrame([[latitude, longitude]], columns=['Latitude', 'Longitude'])\n",
    "    \n",
    "    # Use the model to predict the most common trash item\n",
    "    prediction = model.predict(input_features)\n",
    "    \n",
    "    return prediction[0]\n",
    "\n",
    "# Example coordinates\n",
    "latitude_value = 38.187609  # Replace with actual latitude\n",
    "longitude_value = -121.974758  # Replace with actual longitude\n",
    "\n",
    "# Predict the trash item for the provided coordinates using Logistic Regression model\n",
    "predicted_trash_item = predict_trash_item(latitude_value, longitude_value, logistic_classifier)\n",
    "print(f\"Predicted most common trash item: {predicted_trash_item}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fe3919",
   "metadata": {},
   "source": [
    "# Model 4 :KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e2ea7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the classifier: 39.92%\n",
      "Training Accuracy: 55.09%\n",
      "Validation Accuracy: 39.92%\n",
      "Precision: 37.99%\n",
      "Recall: 39.92%\n",
      "F1 Score: 38.32%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'cleaned_ocean.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Columns to exclude that are not trash items\n",
    "excluded_columns = ['Total Items Collected', 'Adults', 'Children', 'People', 'Pounds', 'Miles', 'Year', 'Day', 'Month', 'Cleanup ID', 'Latitude', 'Longitude', 'Group Name', 'Zone', 'State']\n",
    "\n",
    "# Identify columns related to trash items collected based on data type and excluding the non-trash item columns\n",
    "trash_item_columns = [col for col in data.columns if data[col].dtype in ['int64', 'float64'] and col not in excluded_columns]\n",
    "\n",
    "# Determine the most common trash item for each cleanup event\n",
    "data['Most_Common_Trash'] = data[trash_item_columns].idxmax(axis=1)\n",
    "\n",
    "# Prepare the features and target\n",
    "X = data[['Latitude', 'Longitude']] \n",
    "y = data['Most_Common_Trash']  \n",
    "\n",
    "# Splitting the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the KNN Classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors (k) as needed\n",
    "\n",
    "# Train the classifier\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Precision, Recall, and F1 Score, expressed as a percentage\n",
    "precision = precision_score(y_test, y_pred, average='weighted') * 100  # Multiplied by 100 to convert to percentage\n",
    "recall = recall_score(y_test, y_pred, average='weighted') * 100\n",
    "f1 = f1_score(y_test, y_pred, average='weighted') * 100\n",
    "# Print out the results\n",
    "print(f\"Accuracy of the classifier: {accuracy:.2%}\")\n",
    "training_accuracy = accuracy_score(y_train, knn_classifier.predict(X_train))\n",
    "validation_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Training Accuracy: {training_accuracy:.2%}\")\n",
    "print(f\"Validation Accuracy: {validation_accuracy:.2%}\")\n",
    "\n",
    "print(f\"Precision: {precision:.2f}%\")\n",
    "print(f\"Recall: {recall:.2f}%\")\n",
    "print(f\"F1 Score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6341c676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted most common trash item using KNN: Grocery Bags (Plastic)\n"
     ]
    }
   ],
   "source": [
    "# Function to predict the most common trash item based on given latitude and longitude using KNN\n",
    "def predict_trash_item_knn(latitude, longitude, model):\n",
    "    # Create a DataFrame with the input features\n",
    "    input_features = pd.DataFrame([[latitude, longitude]], columns=['Latitude', 'Longitude'])\n",
    "    \n",
    "    # Use the model to predict the most common trash item\n",
    "    prediction = model.predict(input_features)\n",
    "    \n",
    "    return prediction[0]\n",
    "\n",
    "# Example coordinates\n",
    "latitude_value = 38.187609  # Replace with actual latitude\n",
    "longitude_value = -121.974758  # Replace with actual longitude\n",
    "\n",
    "# Predict the trash item for the provided coordinates using KNN model\n",
    "predicted_trash_item_knn = predict_trash_item_knn(latitude_value, longitude_value, knn_classifier)\n",
    "print(f\"Predicted most common trash item using KNN: {predicted_trash_item_knn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6829ef",
   "metadata": {},
   "source": [
    "# Model 5 :SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b049de09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the classifier: 29.60%\n",
      "Training Accuracy: 29.01%\n",
      "Validation Accuracy: 29.60%\n",
      "Precision: 15.42%\n",
      "Recall: 29.60%\n",
      "F1 Score: 19.83%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'cleaned_ocean.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Columns to exclude that are not trash items\n",
    "excluded_columns = [\n",
    "    'Total Items Collected', 'Adults', 'Children', 'People', 'Pounds', 'Miles', \n",
    "    'Year', 'Day', 'Month', 'Cleanup ID', 'Latitude', 'Longitude', 'Group Name', \n",
    "    'Zone', 'State'\n",
    "]\n",
    "\n",
    "# Identify columns related to trash items collected based on data type and excluding the non-trash item columns\n",
    "trash_item_columns = [col for col in data.columns if data[col].dtype in ['int64', 'float64'] and col not in excluded_columns]\n",
    "\n",
    "# Determine the most common trash item for each cleanup event\n",
    "data['Most_Common_Trash'] = data[trash_item_columns].idxmax(axis=1)\n",
    "\n",
    "# Prepare the features and target\n",
    "X = data[['Latitude', 'Longitude']]  # Features\n",
    "y = data['Most_Common_Trash']  # Target\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Reduce the data using PCA\n",
    "pca = PCA(n_components=2)  # Since we only have 2 features, we'll keep them both\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Splitting the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the SVM Classifier with optimized parameters\n",
    "svm_classifier = SVC(kernel='linear', C=1.0, random_state=42, cache_size=7000)  # Increased cache size\n",
    "\n",
    "# Train the classifier\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print out the results\n",
    "accuracy, training_accuracy, validation_accuracy = (\n",
    "    accuracy_score(y_test, y_pred),\n",
    "    accuracy_score(y_train, svm_classifier.predict(X_train)),\n",
    "    accuracy_score(y_test, y_pred)\n",
    ")\n",
    "\n",
    "accuracy, training_accuracy, validation_accuracy\n",
    "\n",
    "\n",
    "# Calculate Precision, Recall, and F1 Score\n",
    "precision = precision_score(y_test, y_pred, average='weighted') * 100\n",
    "recall = recall_score(y_test, y_pred, average='weighted') * 100\n",
    "f1 = f1_score(y_test, y_pred, average='weighted') * 100\n",
    "\n",
    "# Print out the results with accuracy, precision, recall, and F1 score\n",
    "print(f\"Accuracy of the classifier: {accuracy*100:.2f}%\")\n",
    "print(f\"Training Accuracy: {training_accuracy*100:.2f}%\")\n",
    "print(f\"Validation Accuracy: {validation_accuracy*100:.2f}%\")\n",
    "print(f\"Precision: {precision:.2f}%\")\n",
    "print(f\"Recall: {recall:.2f}%\")\n",
    "print(f\"F1 Score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14d2074e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted most common trash item using SVM: Cigarette Butts\n"
     ]
    }
   ],
   "source": [
    "# Function to predict the most common trash item based on given latitude and longitude using SVM\n",
    "def predict_trash_item_svm(latitude, longitude, model):\n",
    "    # Create a DataFrame with the input features\n",
    "    input_features = pd.DataFrame([[latitude, longitude]], columns=['Latitude', 'Longitude'])\n",
    "    \n",
    "    # Use the model to predict the most common trash item\n",
    "    prediction = model.predict(input_features)\n",
    "    \n",
    "    return prediction[0]\n",
    "\n",
    "# Example coordinates\n",
    "latitude_value = 38.187609  \n",
    "longitude_value = -121.974758  \n",
    "\n",
    "# Predict the trash item for the provided coordinates using SVM model\n",
    "predicted_trash_item_svm = predict_trash_item_svm(latitude_value, longitude_value, svm_classifier)\n",
    "print(f\"Predicted most common trash item using SVM: {predicted_trash_item_svm}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
